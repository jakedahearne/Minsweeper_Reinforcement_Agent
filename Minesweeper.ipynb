{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minesweeper",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz6y1YEDyaDB"
      },
      "source": [
        "# INTRODUCTION\n",
        "\n",
        "This version of our code trains the agent to solve a 9x9 Minesweeper grid with 10 mines. It has issues when it runs in that the agent starts to pick the same action over and over again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aigonsdvCmt"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmYsVljOqybN"
      },
      "source": [
        "#Minesweeper Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUwR6las7FcV"
      },
      "source": [
        "#@title Output { vertical-output: true }\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "UNKNOWN = 9 # unknown (covered) cell is indentified by value 9\n",
        "\n",
        "class MinesweeperEnv:\n",
        "\n",
        "    def __init__(self, rows=9, columns=9, mines=10):\n",
        "\n",
        "        self.rows = rows\n",
        "        self.columns = columns\n",
        "        self.mines = mines\n",
        "        self.grid = None # grid that contains mine locations\n",
        "        self.state = np.full((rows, columns), UNKNOWN)  # state that shows cells as covered or uncovered\n",
        "        self.state_one_hot = (np.arange(UNKNOWN+1) == self.state[...,None]).astype(int) # https://stackoverflow.com/questions/36960320/convert-a-2d-matrix-to-a-3d-one-hot-matrix-numpy\n",
        "        self.reset_step = True\n",
        "        self.unchosen_actions = np.ones((rows, columns)) # array that indicates cells that have been chosen\n",
        "\n",
        "    # creates grid of mines and values indicating neighbouring mines\n",
        "    def createGrid(self, start_row, start_column):\n",
        "        grid = np.zeros((self.rows, self.columns), dtype=int)\n",
        "        mines = self.mines\n",
        "\n",
        "        while mines > 0:\n",
        "            # randomly generate a cell location for the mine\n",
        "            row = np.random.randint(0, self.rows)\n",
        "            column = np.random.randint(0, self.columns)\n",
        "\n",
        "            # if the cell location does not already contain a mine and is not the starting cell\n",
        "            if (grid[row][column] == 0) and (row != start_row and column != start_column) :\n",
        "                # place mine\n",
        "                grid[row][column] = -1\n",
        "                mines -= 1\n",
        "\n",
        "        # fill remaining cells with values indicating neighbouring mines\n",
        "        for row in range(self.rows):\n",
        "            for column in range(self.columns):\n",
        "                if grid[row][column] != -1: # if it is not a mine\n",
        "                    grid[row][column] = self.surroundingMines(row, column, grid)\n",
        "\n",
        "        self.grid = grid.copy()\n",
        "\n",
        "    # counts mines in 8 cells surrounding cell at (row,column) given\n",
        "    def surroundingMines(self, row, column, grid):\n",
        "        mine_count = 0\n",
        "        for r in [row-1, row, row+1]:\n",
        "            for c in [column-1, column, column+1]:\n",
        "                if (0 <= r < self.rows) and (0 <= c < self.columns):\n",
        "                    if grid[r][c] == -1:\n",
        "                        mine_count += 1\n",
        "        return mine_count\n",
        "\n",
        "    # counts the cells that have been uncovered that surround the cell at (row, column) given\n",
        "    def anySurroundingRevealed(self, row, column):\n",
        "        for r in [row-1, row, row+1]:\n",
        "            for c in [column-1, column, column+1]:\n",
        "                if (0 <= r < self.rows) and (0 <= c < self.columns) and not(r == row and c == column):\n",
        "                    if self.state[r][c] != UNKNOWN:\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "    # calculates one hot encoding for value at (row, column) given\n",
        "    def updateOneHot(self, row, column):\n",
        "        one_hot_val = np.zeros(UNKNOWN + 1, dtype=int)\n",
        "        one_hot_val[self.grid[row, column]] = 1\n",
        "        self.state_one_hot[row, column] = one_hot_val\n",
        "\n",
        "    # environment time step\n",
        "    def step(self, action):\n",
        "\n",
        "        terminal = False\n",
        "\n",
        "        # if this is the first move of the game: create board\n",
        "        if self.reset_step:\n",
        "            self.createGrid(action[0], action[1])\n",
        "        \n",
        "        # if action has already been chosen\n",
        "        elif self.state[action[0], action[1]] != UNKNOWN:\n",
        "            reward = -0.3\n",
        "            return reward, terminal # don't need to update state or unchosen actions\n",
        "\n",
        "        # update state representation with value in grid\n",
        "        self.state[action[0], action[1]] = self.grid[action[0], action[1]]\n",
        "\n",
        "        # if this is the first move of the game: \n",
        "        if self.reset_step:\n",
        "            self.reset_step = False\n",
        "            self.updateOneHot(action[0], action[1])\n",
        "            reward = -0.3\n",
        "\n",
        "        # if the action reveals a mine\n",
        "        elif self.grid[action[0]][action[1]] == -1:\n",
        "            self.reset()\n",
        "            reward = -1\n",
        "            terminal = True\n",
        "\n",
        "        # if the game is in a winning state\n",
        "        elif self.win():\n",
        "            self.reset()\n",
        "            reward = 1\n",
        "            terminal = True\n",
        "\n",
        "        # if the action taken already has surrounding cells revealed\n",
        "        elif self.anySurroundingRevealed(action[0], action[1]):\n",
        "            self.updateOneHot(action[0], action[1])\n",
        "            reward = 0.3\n",
        "\n",
        "        # if the action taken has no surrounding cells revealed\n",
        "        else:\n",
        "            self.updateOneHot(action[0], action[1])\n",
        "            reward = -0.3\n",
        "\n",
        "        # set action as chosen\n",
        "        self.unchosen_actions[action[0], action[1]] = 0\n",
        "\n",
        "        return reward, terminal\n",
        "\n",
        "    # determines if in winning state\n",
        "    def win(self):\n",
        "        covered = np.where(self.state == UNKNOWN)\n",
        "        mines = np.where(self.grid == -1)\n",
        "        if list(zip(covered[0], covered[1])) == list(zip(mines[0], mines[1])): # if the only covered cells are found at the same cells as mines\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # reset environment\n",
        "    def reset(self):\n",
        "        self.reset_step = True\n",
        "        self.grid = None\n",
        "        self.state = np.full((self.rows, self.columns), UNKNOWN)\n",
        "        self.state_one_hot = (np.arange(UNKNOWN+1) == self.state[...,None]).astype(int)\n",
        "        self.unchosen_actions = np.ones((self.rows, self.columns))\n",
        "\n",
        "    # draw environment\n",
        "    def drawEnv(self):\n",
        "        moves = 0\n",
        "        data = np.full((self.rows, self.columns),2)\n",
        "        fig, ax = plt.subplots()\n",
        "        for i in range(self.rows+1):\n",
        "          ax.axhline(i, lw=2, color='k')\n",
        "          ax.axvline(i, lw=2, color='k')\n",
        "        ax.imshow(data, cmap=\"Greys\", vmin=0, vmax=5, extent=[0, 9, 9, 0])\n",
        "        ax.axis('off')\n",
        "        display(fig)\n",
        "        return fig, ax, data, moves\n",
        "  \n",
        "    # update drawing of environment\n",
        "    def updateDrawing(self, fig, ax, data, moves, x, y):\n",
        "        moves += 1\n",
        "        if self.grid is None:\n",
        "          if moves == (self.rows*self.columns)-self.mines:\n",
        "            print(\"WON: all cells uncovered\")\n",
        "            ax.text(y+0.2,x+0.5,\"WON\",c=\"green\")\n",
        "          else:\n",
        "            print(\"LOST: hit a mine at location\", (x,y))\n",
        "            ax.text(y+0.5,x+0.5,\"M\",c=\"red\")\n",
        "        else:\n",
        "          ax.text(y+0.5,x+0.5,self.grid[x,y])\n",
        "        data[x,y] = 1\n",
        "        ax.imshow(data, cmap=\"Greys\", vmin=0, vmax=5, extent=[0, 9, 9, 0])\n",
        "        display(fig)\n",
        "        return fig, ax, data, moves\n",
        "\n",
        "\n",
        "# This is to manually test the environment\n",
        "'''\n",
        "env = MinesweeperEnv()\n",
        "fig, ax, data, moves = env.drawEnv()\n",
        "x = int(input('Enter x value: '))\n",
        "y = int(input('Enter y value: '))\n",
        "while x != -1:\n",
        "    print('Reward: ', env.step((x, y)))\n",
        "    print('Grid:\\n', env.grid)\n",
        "    print('What agent sees:\\n', env.state)\n",
        "    fig, ax, data, moves = env.updateDrawing(fig, ax, data, moves, x, y)\n",
        "    x = int(input('Enter x value: '))\n",
        "    y = int(input('Enter y value: '))\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ9a_DtUq6d9"
      },
      "source": [
        "#Deep Q-Learning Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvJDTEVyq-s6"
      },
      "source": [
        "from collections import deque\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import random\n",
        "\n",
        "GAMMA = 0.99\n",
        "MINIBATCH_SIZE = 32\n",
        "TARGET_MODEL_UPDATE_TIME = 10\n",
        "MIN_REPLAY_MEMORY_SIZE = 1000\n",
        "\n",
        "class DQAgent:\n",
        "\n",
        "  def __init__(self, rows=9, columns=9): #initialize replay memory, main nn and target nn\n",
        "    self.rows = rows\n",
        "    self.columns = columns\n",
        "    \n",
        "    self.replayMem = deque(maxlen=10000)\n",
        "    self.epsilon = 0.1\n",
        "    self.learningRate = 0.001\n",
        "\n",
        "    self.mainModel = self.createModel()\n",
        "    self.targetModel = self.createModel()\n",
        "    self.targetModel.set_weights(self.mainModel.get_weights())\n",
        "\n",
        "    self.targetModelUpdateCounter = 0\n",
        "\n",
        "  # create neural network for q-values\n",
        "  def createModel(self):\n",
        "    model = Sequential([\n",
        "                Conv2D(128, (3,3), activation='relu', padding='same', input_shape=(9, 9, 10)),\n",
        "                Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "                Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "                Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "                Flatten(),\n",
        "                Dense(512, activation='relu'),\n",
        "                Dense(512, activation='relu'),\n",
        "                Dense(81, activation='linear')])\n",
        "    \n",
        "\n",
        "    # Previously tried neural network setup\n",
        "    '''\n",
        "    init = tf.keras.initializers.RandomNormal(mean = 0.5, stddev=0.5)\n",
        "\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(keras.layers.Dense(810, input_shape=(9,9,10), activation='relu', kernel_initializer=init)) #self.rows * self.columns * 10\n",
        "    model.add(keras.layers.Dense(256, activation='relu', kernel_initializer=init)) #round(np.sqrt(self.rows**2 * self.columns**2 * 10))\n",
        "    model.add(Flatten())\n",
        "    model.add(keras.layers.Dense(81, activation='linear', kernel_initializer=init)) #self.rows * self.columns\n",
        "    '''\n",
        "\n",
        "    # displays model summary\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=self.learningRate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  # adds new transition to replay memory\n",
        "  def updateReplayMemory(self, currentState, action, reward, nextState, terminalState):\n",
        "    self.replayMem.append((currentState, action, reward, nextState, terminalState))\n",
        "\n",
        "  # training step for agent\n",
        "  def train(self, terminal):\n",
        "\n",
        "    # replay memory must be at least minimum replay memory size before we start sampling from it\n",
        "    if len(self.replayMem) < MIN_REPLAY_MEMORY_SIZE:\n",
        "      return\n",
        "\n",
        "    # get minibatch of (state, action, reward, next_state) from replay memory\n",
        "    minibatch = random.sample(self.replayMem, MINIBATCH_SIZE)\n",
        "\n",
        "    #create list containing each state from each minibatch\n",
        "    #create list containing each next state from minibatch\n",
        "    currentStates = []\n",
        "    nextStates = []\n",
        "    for sample in minibatch:\n",
        "      currentStates.append(sample[0])\n",
        "      nextStates.append(sample[3])\n",
        "\n",
        "    # query main nn for q values of current states\n",
        "    currentQVals = self.mainModel.predict(np.array(currentStates))\n",
        "    # query target nn for q values of next states\n",
        "    nextQVals = self.targetModel.predict(np.array(nextStates))\n",
        "\n",
        "    states = []\n",
        "    updatedQVals = []\n",
        "\n",
        "    # for each transition in the minibatch\n",
        "    for index, (currentState, action, reward, nextState, terminalState) in enumerate(minibatch):\n",
        "      # if next state is a winning game or losing game, update q value with just reward\n",
        "      if terminalState:\n",
        "        updatedQVal = reward \n",
        "      else:\n",
        "        # otherwise update q value with q-learning update using q values of next states\n",
        "        updatedQVal = reward + (GAMMA * np.max(nextQVals[index]))\n",
        "\n",
        "      currentQsForState = currentQVals[index]\n",
        "      currentQsForState[(action[0] * self.columns) + action[1]] = updatedQVal #updates the q val for the current state, current action\n",
        "\n",
        "      states.append(currentState)\n",
        "      updatedQVals.append(currentQsForState)\n",
        "\n",
        "    # fit main nn with current states and updated q values\n",
        "    self.mainModel.fit(np.array(states), np.array(updatedQVals), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False) \n",
        "    \n",
        "    if terminal:\n",
        "      self.targetModelUpdateCounter += 1\n",
        "\n",
        "    # update target nn as main nn if it is time to do so\n",
        "    if self.targetModelUpdateCounter > TARGET_MODEL_UPDATE_TIME:\n",
        "      self.targetModel.set_weights(self.mainModel.get_weights())\n",
        "      self.targetModelUpdateCounter = 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjP82racVaCf"
      },
      "source": [
        "###Algorithm for deep Q Networks: (From [DQN Nature Paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf))\n",
        "\n",
        "```\n",
        "Initialize replay memory D to capacity N\n",
        "Initialize action-value function Q with random weights Œ∏\n",
        "Initialize target action-value function Q* with weights Œ∏ÃÑ  = Œ∏\n",
        "For episode = 1,M do\n",
        "  Initialize sequence s1 = {x1} and preprocessed sequence œÜ1 = œÜ(s1)  #preprocess NN inputs\n",
        "  For t = 1,T do\n",
        "    With probability ùúÄ select a random action a‚Çú\n",
        "    otherwise select a‚Çú = argmax‚ÇêQ(œÜ(s‚Çú), a; Œ∏)\n",
        "    Execute action a‚Çú in emulator and observe reward r‚Çú (and image x‚Çô‚Çë‚Çì‚Çú ‚Çú)  #image is vector (pixels)\n",
        "    Set s‚Çô‚Çë‚Çì‚Çú ‚Çú = s‚Çú,a‚Çú,x‚Çô‚Çë‚Çì‚Çú ‚Çú and preprocess œÜ‚Çô‚Çë‚Çì‚Çú ‚Çú = œÜ(s‚Çô‚Çë‚Çì‚Çú ‚Çú)\n",
        "    Store transition (œÜ‚Çú, a‚Çú, r‚Çú, œÜ‚Çô‚Çë‚Çì‚Çú ‚Çú) in D\n",
        "\n",
        "```\n",
        "**Agent update step:**\n",
        "\n",
        "        Sample random minibatch of transitions (œÜ‚±º, a‚±º, r‚±º, œÜ‚Çô‚Çë‚Çì‚Çú ‚±º) from D\n",
        "        Set y‚±º = r‚±º if episode terminates at step (next j)  #y‚±º is Q-value\n",
        "        otherwise set y‚±º = r‚±º + Œ≥max‚Çê(œÜ‚Çô‚Çë‚Çì‚Çú ‚±º, a'; Œ∏ÃÑ )\n",
        "        Perform a gradient descent step on (y‚±º - Q(œÜ‚±º, a‚±º; Œ∏))^2 with respect to the network parameters Œ∏\n",
        "        Every C steps reset Q* = Q **\n",
        "\n",
        "```\n",
        "  End For\n",
        "End For\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGJPkOlLTnZn"
      },
      "source": [
        "#Execution Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qxKfL3LTr-o"
      },
      "source": [
        "# determines which action the agent chooses by epsilon-greedy\n",
        "def chooseAction(agent, state, unchosen):\n",
        "  \n",
        "  # choose random action if random value is less than epsilon\n",
        "  if np.random.random() < agent.epsilon:\n",
        "    actions = np.argwhere(unchosen) # get indices of all unchosen cells\n",
        "    index = np.random.randint(0,len(actions)) # generate a random index\n",
        "    action = actions[index] # get unchosen action as that index\n",
        "\n",
        "  # choose optimal action according to model\n",
        "  else:\n",
        "    pred = agent.mainModel.predict(np.array(state).reshape(-1, *state.shape)) #NOTE: we believe this could have caused the repeated action picking issue\n",
        "    unchosen = unchosen.reshape(1, 81) \n",
        "    pred[unchosen == 0] = np.min(pred) # set the recieved q-values for all chosen cells as the minimum q-value so they don't get chosen as the optimal \n",
        "    index = np.argmax(pred) # get index of optimal action\n",
        "    action = convertIndexToAction(index, agent)\n",
        "  return action\n",
        "\n",
        "\n",
        "# converts the index from the (1,81) 1D array to an index for a (9,9) 2D array\n",
        "def convertIndexToAction(index, agent):\n",
        "  row = math.floor(index / agent.columns)\n",
        "  column = index - (row * agent.columns)\n",
        "  return (row, column)\n",
        "\n",
        "\n",
        "# trains agent for a number of episodes\n",
        "def train(agent, env, num_episodes=10000):\n",
        "  rewards = []\n",
        "  episodes = []\n",
        "  steps = []\n",
        "\n",
        "  #loop through episodes\n",
        "  for episode in range(num_episodes):\n",
        "    step = 0\n",
        "\n",
        "    #get initial state\n",
        "    current_state = env.state_one_hot\n",
        "\n",
        "    terminal = False\n",
        "    sumRewards = 0\n",
        "\n",
        "    #loop through step in episode\n",
        "    while not terminal:\n",
        "      step += 1\n",
        "\n",
        "      #choose next action based on current state\n",
        "      action = chooseAction(agent, current_state, env.unchosen_actions)\n",
        "\n",
        "      #execute action with env.step(action) and get reward and next_state\n",
        "      reward, terminal = env.step(action)\n",
        "      sumRewards += reward\n",
        "      next_state = env.state_one_hot\n",
        "\n",
        "      #update agent's replay memory with (current_state, action, reward, next_state)\n",
        "      agent.updateReplayMemory(current_state, action, reward, next_state, terminal)\n",
        "\n",
        "      #train agent\n",
        "      agent.train(terminal)\n",
        "\n",
        "      current_state = next_state\n",
        "\n",
        "    print(episode) # indicates episode number\n",
        "\n",
        "    rewards.append(sumRewards)\n",
        "    episodes.append(episode)\n",
        "    steps.append(step)\n",
        "  \n",
        "  # creates plots\n",
        "  plt.clf()\n",
        "  plt.plot(episodes,rewards)\n",
        "  plt.show()\n",
        "  plt.plot(episodes, steps)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "#evaluates agent for a number of episodes\n",
        "def evaluate(agent, env, num_episodes=1):\n",
        "  agent.epsilon = 0 # no random choice in evaluation\n",
        "\n",
        "  #loop through episodes\n",
        "  for episode in range(num_episodes):\n",
        "\n",
        "    #get initial state\n",
        "    current_state = env.state_one_hot\n",
        "    fig, ax, data, moves = env.drawEnv()\n",
        "    terminal = False\n",
        "\n",
        "    #loop through step in episode\n",
        "    while not terminal:\n",
        "\n",
        "      #choose next action based on current state\n",
        "      action = chooseAction(agent, current_state, env.unchosen_actions)\n",
        "      print(action)\n",
        "\n",
        "      #execute action with env.step(action) and get reward and next_state\n",
        "      reward, terminal = env.step(action)\n",
        "      fig, ax, data, moves = env.updateDrawing(fig, ax, data, moves, action[0], action[1])\n",
        "      next_state = env.state_one_hot\n",
        "\n",
        "      current_state = next_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VLI-h8nBX2PP"
      },
      "source": [
        "dq_agent = DQAgent()\n",
        "environment = MinesweeperEnv()\n",
        "\n",
        "train(dq_agent, environment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA-7a-cPcbHG"
      },
      "source": [
        "evaluate(dq_agent, environment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqikYyk39Vlv"
      },
      "source": [
        "https://github.com/jakejhansen/minesweeper_solver \n",
        "\n",
        "https://sdlee94.github.io/Minesweeper-AI-Reinforcement-Learning/ \n",
        "\n",
        "https://hanialmousli.wordpress.com/2017/10/11/minesweeper-using-reinforcement-learning/\n",
        "\n",
        "https://www.researchgate.net/profile/Preslav-Nakov/publication/228613592_Minesweeper_Minesweeper/links/00b7d523c1308589ef000000/Minesweeper-Minesweeper.pdf\n",
        "\n",
        "http://volweb.utk.edu/~mmajumde/research_web/ECE517final.pdf\n",
        "\n",
        "https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial/?completed=/deep-q-learning-dqn-reinforcement-learning-python-tutorial/\n",
        "\n",
        "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"
      ]
    }
  ]
}